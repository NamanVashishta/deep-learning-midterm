{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"93fa20a3c81c4c599b6189371c356964":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8022dcb540b84020a2776cbee132ab5f","IPY_MODEL_9888010599a14ee386d57c76d0880373","IPY_MODEL_e91b94c99a954fe0a7a52854fa19b291"],"layout":"IPY_MODEL_a375c537a6da47eea6e62fffbd6c3629"}},"8022dcb540b84020a2776cbee132ab5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_732e0544d9af41d7aa614f9f1a445830","placeholder":"​","style":"IPY_MODEL_3f108398683a447baa58bc25816c610f","value":"model.safetensors: 100%"}},"9888010599a14ee386d57c76d0880373":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72bbc6abf9c34f00b0da8b780714b9f1","max":5964186429,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5e94ec64c808496885f2912228428c50","value":5964186429}},"e91b94c99a954fe0a7a52854fa19b291":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_decf73ae93a7406a9eb0f0181bf2b8e7","placeholder":"​","style":"IPY_MODEL_936843efe9fd4cd989c532cbaca23c7b","value":" 5.96G/5.96G [01:05&lt;00:00, 251MB/s]"}},"a375c537a6da47eea6e62fffbd6c3629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"732e0544d9af41d7aa614f9f1a445830":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f108398683a447baa58bc25816c610f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72bbc6abf9c34f00b0da8b780714b9f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e94ec64c808496885f2912228428c50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"decf73ae93a7406a9eb0f0181bf2b8e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"936843efe9fd4cd989c532cbaca23c7b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d3766a8a32f478089d2ad49b3717e54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9509e60c084c4b11bcbcff8cda10934e","IPY_MODEL_dc074d92cb484cce8483397ae391ea12","IPY_MODEL_3203313c785446c6bb9b944412645f79"],"layout":"IPY_MODEL_0b1bfe8f662f435289ebbd229c0fe587"}},"9509e60c084c4b11bcbcff8cda10934e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3827bf23f4f42a296800b888ed986ed","placeholder":"​","style":"IPY_MODEL_a32778c9944b4dbca16664415117d631","value":"generation_config.json: 100%"}},"dc074d92cb484cce8483397ae391ea12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b29a4e384fb46ef877a7fa99ea58bd5","max":235,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e786f0d634245f799a8c3b321ed55f8","value":235}},"3203313c785446c6bb9b944412645f79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73910176b15c497cb1cb79a9cd35b297","placeholder":"​","style":"IPY_MODEL_c3fa7eb7b5b74085a1566f0e2022ab00","value":" 235/235 [00:00&lt;00:00, 28.0kB/s]"}},"0b1bfe8f662f435289ebbd229c0fe587":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3827bf23f4f42a296800b888ed986ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a32778c9944b4dbca16664415117d631":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b29a4e384fb46ef877a7fa99ea58bd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e786f0d634245f799a8c3b321ed55f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73910176b15c497cb1cb79a9cd35b297":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3fa7eb7b5b74085a1566f0e2022ab00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ff927cff9e14892a7faf43ca5d8a08f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2655d0af5cbc4b4789399b6b72e23027","IPY_MODEL_53751abd75994b3ebc4965af899bd1d1","IPY_MODEL_f623832605da425293d4ec72b587896f"],"layout":"IPY_MODEL_49b0281c4e4d4240a48a189e8f56d453"}},"2655d0af5cbc4b4789399b6b72e23027":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_159efc4772664ed9a163be6747377249","placeholder":"​","style":"IPY_MODEL_3adbb7169557439a8fd9743d9c519cb2","value":"tokenizer_config.json: "}},"53751abd75994b3ebc4965af899bd1d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ee87881285344e48ba1abbd567bd862","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26526e44fa83434f938e984d1d74cb14","value":1}},"f623832605da425293d4ec72b587896f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98466f3efe1d46d1b3cfa3f2778adb64","placeholder":"​","style":"IPY_MODEL_c11d91d8749f42f880198405afbf61ce","value":" 50.6k/? [00:00&lt;00:00, 4.44MB/s]"}},"49b0281c4e4d4240a48a189e8f56d453":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"159efc4772664ed9a163be6747377249":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3adbb7169557439a8fd9743d9c519cb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ee87881285344e48ba1abbd567bd862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"26526e44fa83434f938e984d1d74cb14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98466f3efe1d46d1b3cfa3f2778adb64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c11d91d8749f42f880198405afbf61ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3cf4b5bd0594e7a8257e5759bbd8876":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e00ad2dd23b34ac9bbea4606fbd36d3b","IPY_MODEL_534285da8332487d9f4ad5edcd94435d","IPY_MODEL_fe800e6ed65642e4bd7c7b0089affaac"],"layout":"IPY_MODEL_1ed30e1f9b354bb193c4b7218bf3e466"}},"e00ad2dd23b34ac9bbea4606fbd36d3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c38247fddba4018a196ae3911976fce","placeholder":"​","style":"IPY_MODEL_8879f73f28ea4f05b5f175684c73caa5","value":"special_tokens_map.json: 100%"}},"534285da8332487d9f4ad5edcd94435d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ad027ab3fb94b33af9146e86c651a2f","max":459,"min":0,"orientation":"horizontal","style":"IPY_MODEL_572f0050e45745ca8077fdfc3e3a8bde","value":459}},"fe800e6ed65642e4bd7c7b0089affaac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da102703e9de44c5b54ea718ac7ea038","placeholder":"​","style":"IPY_MODEL_6dbdcd5d16ab46f69d019e68c22c9740","value":" 459/459 [00:00&lt;00:00, 48.5kB/s]"}},"1ed30e1f9b354bb193c4b7218bf3e466":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c38247fddba4018a196ae3911976fce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8879f73f28ea4f05b5f175684c73caa5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ad027ab3fb94b33af9146e86c651a2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"572f0050e45745ca8077fdfc3e3a8bde":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da102703e9de44c5b54ea718ac7ea038":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dbdcd5d16ab46f69d019e68c22c9740":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bdb32dda9e7495191061d130b5757cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9dc9279f28214300a0857f62eb98ac99","IPY_MODEL_e50a48034bcb4769ba1a321a89d7a52d","IPY_MODEL_a5964859bc0543bdada2c860531e0868"],"layout":"IPY_MODEL_0aae35b9e8ae4db49f664d92ad2fb8fb"}},"9dc9279f28214300a0857f62eb98ac99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3802d5034c984b8dac325f6c03de0619","placeholder":"​","style":"IPY_MODEL_38f3f2414a8747688c68f42e6b31de23","value":"tokenizer.json: 100%"}},"e50a48034bcb4769ba1a321a89d7a52d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_928c60b0f4c34152a196323bb21b1c03","max":17209920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7973134640f342e5b1740c5d53386020","value":17209920}},"a5964859bc0543bdada2c860531e0868":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_678a98b5eba24460a5495a9a21fca106","placeholder":"​","style":"IPY_MODEL_9699670d5dc14fd78842df0f47a70570","value":" 17.2M/17.2M [00:00&lt;00:00, 32.6MB/s]"}},"0aae35b9e8ae4db49f664d92ad2fb8fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3802d5034c984b8dac325f6c03de0619":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38f3f2414a8747688c68f42e6b31de23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"928c60b0f4c34152a196323bb21b1c03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7973134640f342e5b1740c5d53386020":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"678a98b5eba24460a5495a9a21fca106":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9699670d5dc14fd78842df0f47a70570":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3fc0093e28cd4871a855aac834aadcb4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4784c377c0a4f0f8ff15c35889d5e6b","IPY_MODEL_d99f16a4e908473e897000d33c05433c","IPY_MODEL_698b2a3391254e44b9babbdf0163e440"],"layout":"IPY_MODEL_feca082aa72e47848d7a8cf7063991ce"}},"e4784c377c0a4f0f8ff15c35889d5e6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ed447e34b4d4a8996c3432dcd2241b3","placeholder":"​","style":"IPY_MODEL_b44ca299d2ed41c99361178b32d3d2fc","value":"Map (num_proc=1): 100%"}},"d99f16a4e908473e897000d33c05433c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8238aaa6ebf4db4baa2ac190e01a055","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a97f3b7906864868b084ad778c6b5458","value":2000}},"698b2a3391254e44b9babbdf0163e440":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86d98b810c8149e6bbabf3ea4487488a","placeholder":"​","style":"IPY_MODEL_ce864fb137a442dc88c4356247cde238","value":" 2000/2000 [00:01&lt;00:00, 2517.21 examples/s]"}},"feca082aa72e47848d7a8cf7063991ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ed447e34b4d4a8996c3432dcd2241b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b44ca299d2ed41c99361178b32d3d2fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8238aaa6ebf4db4baa2ac190e01a055":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a97f3b7906864868b084ad778c6b5458":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86d98b810c8149e6bbabf3ea4487488a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce864fb137a442dc88c4356247cde238":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(\"Cell 2: Installing unsloth and forcing upgrade on all dependencies...\")\n\n!pip install \"unsloth[colab-new]\"\n\nprint(\"Cell 2 Complete: Unsloth and all dependencies force-upgraded.\")\nprint(\"!!! IMPORTANT: YOU MUST RESTART THE KERNEL NOW BEFORE RUNNING THE NEXT CELL. !!!\")","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"5idYHo7SE8cA","outputId":"cad17a12-d972-4875-8780-8bde9c7b388d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cell 2: Installing unsloth and forcing upgrade on all dependencies...\n","Collecting unsloth[colab-new]\n","  Downloading unsloth-2025.10.12-py3-none-any.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting unsloth_zoo>=2025.10.13 (from unsloth[colab-new])\n","  Downloading unsloth_zoo-2025.10.13-py3-none-any.whl.metadata (32 kB)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.45.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (25.0)\n","Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.23.0+cu126)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (2.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (5.9.5)\n","Collecting tyro (from unsloth[colab-new])\n","  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (5.29.5)\n","Collecting xformers>=0.0.27.post2 (from unsloth[colab-new])\n","  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n","Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth[colab-new])\n","  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (3.4.0)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.2.1)\n","Collecting datasets!=4.0.*,!=4.1.0,>=3.4.1 (from unsloth[colab-new])\n","  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (1.11.0)\n","Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.17.1)\n","Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.36.0)\n","Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.1.9)\n","Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (0.35.2)\n","Requirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth[colab-new]) (4.57.1)\n","Collecting trl!=0.19.0,<=0.23.0,>=0.18.2 (from unsloth[colab-new])\n","  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth[colab-new]) (6.0.3)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth[colab-new]) (0.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (3.20.0)\n","Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new])\n","  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n","Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (2.32.4)\n","Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (0.28.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth[colab-new]) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth[colab-new]) (1.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth[colab-new]) (1.11.1.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth[colab-new]) (2024.11.6)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.2,>=4.51.3->unsloth[colab-new]) (0.22.1)\n","Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.10.13->unsloth[colab-new])\n","  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n","Collecting cut_cross_entropy (from unsloth_zoo>=2025.10.13->unsloth[colab-new])\n","  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.10.13->unsloth[colab-new]) (11.3.0)\n","Collecting msgspec (from unsloth_zoo>=2025.10.13->unsloth[colab-new])\n","  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth[colab-new]) (8.7.0)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth[colab-new]) (0.17.0)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth[colab-new]) (13.9.4)\n","Collecting shtab>=1.5.6 (from tyro->unsloth[colab-new])\n","  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth[colab-new]) (4.4.4)\n","\u001b[33mWARNING: unsloth 2025.10.12 does not provide the extra 'triton'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (3.13.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (4.11.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (0.16.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (2.5.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (2.19.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth[colab-new]) (1.3.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth[colab-new]) (3.23.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth[colab-new]) (3.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (1.22.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (1.17.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth[colab-new]) (1.3.1)\n","Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unsloth_zoo-2025.10.13-py3-none-any.whl (273 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.6/273.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unsloth-2025.10.12-py3-none-any.whl (348 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.7/348.7 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n","Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n","Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchao, shtab, pyarrow, msgspec, tyro, xformers, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n","  Attempting uninstall: torchao\n","    Found existing installation: torchao 0.10.0\n","    Uninstalling torchao-0.10.0:\n","      Successfully uninstalled torchao-0.10.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 18.1.0\n","    Uninstalling pyarrow-18.1.0:\n","      Successfully uninstalled pyarrow-18.1.0\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 4.0.0\n","    Uninstalling datasets-4.0.0:\n","      Successfully uninstalled datasets-4.0.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n","cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bitsandbytes-0.48.2 cut_cross_entropy-25.1.1 datasets-4.3.0 msgspec-0.19.0 pyarrow-22.0.0 shtab-1.7.2 torchao-0.14.1 trl-0.23.0 tyro-0.9.35 unsloth-2025.10.12 unsloth_zoo-2025.10.13 xformers-0.0.32.post2\n","Cell 2 Complete: Unsloth and all dependencies force-upgraded.\n","!!! IMPORTANT: YOU MUST RESTART THE KERNEL NOW BEFORE RUNNING THE NEXT CELL. !!!\n"]}],"execution_count":1},{"cell_type":"code","source":"import os\n\n# --- Environment Check ---\nif 'COLAB_GPU' in os.environ:\n    print(\"Running in Google Colab.\")\n    SAVE_PATH = \"/content/drive/MyDrive/llama3_math_checkpoint\"\n\n# We check if the /kaggle/input directory exists. If it does, we're in a Kaggle Notebook.\nelif os.path.exists('/kaggle/input'):\n    print(\"Running in Kaggle Notebook.\")\n    SAVE_PATH = \"/kaggle/working/llama3_math_checkpoint\"\n\nelse:\n    print(\"Running in a local environment.\")\n    SAVE_PATH = \"llama3_math_checkpoint\"\n\n# --- Create Save Directory ---\nos.makedirs(SAVE_PATH, exist_ok=True)\n\nprint(f\"Post-training model checkpoint location set to: {SAVE_PATH}\")\nprint(\"Cell 4 Complete: Environment check done.\")","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"c9BpNz7oE8cB","outputId":"f070e832-2410-4c29-aa3e-c358701967e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running in Google Colab.\n","Post-training model checkpoint location set to: /content/drive/MyDrive/llama3_math_checkpoint\n","Cell 4 Complete: Environment check done.\n"]}],"execution_count":3},{"cell_type":"code","source":"print(\"Cell 5: Importing libraries...\")\n# --- Unsloth ---\nfrom unsloth import FastLanguageModel\n\n# --- PyTorch ---\nimport torch  # The core deep learning library\n\n# --- Hugging Face Datasets ---\nfrom datasets import load_dataset # Used to load the competition dataset from the Hugging Face Hub\n\n# --- Hugging Face TRL & Transformers ---\nfrom transformers import AutoTokenizer\n\n# --- Data Handling ---\nimport pandas as pd \nfrom tqdm import tqdm  \n\n# --- Garbage Collector for memory management later ---\nimport gc\n\nprint(\"Cell 5 Complete: Necessary libraries imported.\")","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"QsG_a8tNE8cB","outputId":"d1877fa7-64ab-403e-a4cb-91e49105097f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cell 5: Importing libraries...\n","🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"]},{"output_type":"stream","name":"stdout","text":["🦥 Unsloth Zoo will now patch everything to make training faster!\n","Cell 5 Complete: Necessary libraries imported.\n"]}],"execution_count":4},{"cell_type":"code","source":"# --- Model Configuration ---\n\n# The maximum number of tokens in a sequence.\nmax_seq_length = 1024\n\n# This will automatically detect the best data type (like float16) for your GPU.\ndtype = None\n\n# This is the magic! Load the model in 4-bit precision (QLoRA).\nload_in_4bit = True\n\n# --- Load the Model ---\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n\n    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\nprint(\"Cell 6 Complete: Base Model and Tokenizer loaded.\")","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":298,"referenced_widgets":["93fa20a3c81c4c599b6189371c356964","8022dcb540b84020a2776cbee132ab5f","9888010599a14ee386d57c76d0880373","e91b94c99a954fe0a7a52854fa19b291","a375c537a6da47eea6e62fffbd6c3629","732e0544d9af41d7aa614f9f1a445830","3f108398683a447baa58bc25816c610f","72bbc6abf9c34f00b0da8b780714b9f1","5e94ec64c808496885f2912228428c50","decf73ae93a7406a9eb0f0181bf2b8e7","936843efe9fd4cd989c532cbaca23c7b","3d3766a8a32f478089d2ad49b3717e54","9509e60c084c4b11bcbcff8cda10934e","dc074d92cb484cce8483397ae391ea12","3203313c785446c6bb9b944412645f79","0b1bfe8f662f435289ebbd229c0fe587","e3827bf23f4f42a296800b888ed986ed","a32778c9944b4dbca16664415117d631","4b29a4e384fb46ef877a7fa99ea58bd5","7e786f0d634245f799a8c3b321ed55f8","73910176b15c497cb1cb79a9cd35b297","c3fa7eb7b5b74085a1566f0e2022ab00","0ff927cff9e14892a7faf43ca5d8a08f","2655d0af5cbc4b4789399b6b72e23027","53751abd75994b3ebc4965af899bd1d1","f623832605da425293d4ec72b587896f","49b0281c4e4d4240a48a189e8f56d453","159efc4772664ed9a163be6747377249","3adbb7169557439a8fd9743d9c519cb2","1ee87881285344e48ba1abbd567bd862","26526e44fa83434f938e984d1d74cb14","98466f3efe1d46d1b3cfa3f2778adb64","c11d91d8749f42f880198405afbf61ce","b3cf4b5bd0594e7a8257e5759bbd8876","e00ad2dd23b34ac9bbea4606fbd36d3b","534285da8332487d9f4ad5edcd94435d","fe800e6ed65642e4bd7c7b0089affaac","1ed30e1f9b354bb193c4b7218bf3e466","6c38247fddba4018a196ae3911976fce","8879f73f28ea4f05b5f175684c73caa5","0ad027ab3fb94b33af9146e86c651a2f","572f0050e45745ca8077fdfc3e3a8bde","da102703e9de44c5b54ea718ac7ea038","6dbdcd5d16ab46f69d019e68c22c9740","6bdb32dda9e7495191061d130b5757cf","9dc9279f28214300a0857f62eb98ac99","e50a48034bcb4769ba1a321a89d7a52d","a5964859bc0543bdada2c860531e0868","0aae35b9e8ae4db49f664d92ad2fb8fb","3802d5034c984b8dac325f6c03de0619","38f3f2414a8747688c68f42e6b31de23","928c60b0f4c34152a196323bb21b1c03","7973134640f342e5b1740c5d53386020","678a98b5eba24460a5495a9a21fca106","9699670d5dc14fd78842df0f47a70570"]},"id":"1zM0ftJ_E8cC","outputId":"d5b5235d-a049-4b27-b3ad-8a641114a810"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93fa20a3c81c4c599b6189371c356964"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d3766a8a32f478089d2ad49b3717e54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ff927cff9e14892a7faf43ca5d8a08f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3cf4b5bd0594e7a8257e5759bbd8876"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bdb32dda9e7495191061d130b5757cf"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cell 6 Complete: Base Model and Tokenizer loaded.\n"]}],"execution_count":5},{"cell_type":"code","source":"# --- LoRA Configuration ---\n# 'FastLanguageModel.get_peft_model' injects the LoRA adapters into our model.\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    lora_alpha = 32,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\", # Use string \"unsloth\"\n    random_state = 42,\n)\n\nprint(\"Cell 7 Complete: LoRA configured (for inspection).\")","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"j0YoOBThE8cC","outputId":"c3a2b3ed-b4a1-40ff-fb13-9aaf273143ca"},"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth 2025.10.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"]},{"output_type":"stream","name":"stdout","text":["Cell 7 Complete: LoRA configured (for inspection).\n"]}],"execution_count":6},{"cell_type":"code","source":"# --- 1. Load the Dataset ---\ndataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n\n# --- 2. Create the Subset ---\nshuffled_dataset = dataset.shuffle(seed=42)\n\ntrain_subset = shuffled_dataset.select(range(2000))\nprint(f\"Using a subset of {len(train_subset)} examples.\")\n\n\n# -----------------------------------------------------------------\n# --- DEBUG: PRINT THE RAW DATA FOR THE FIRST EXAMPLE ---\n# -----------------------------------------------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"--- DEBUG: RAW DATA (EXAMPLE 0) ---\")\nprint(\"=\"*50 + \"\\n\")\nexample = train_subset[0]\nprint(f\"QUESTION:\\n{example['question']}\\n\")\nprint(f\"EXPECTED ANSWER:\\n{example['answer']}\\n\")\nprint(f\"PROVIDED SOLUTION:\\n{example['solution']}\\n\")\nprint(f\"IS_CORRECT (Target Label):\\n{example['is_correct']}\\n\")\n\n\n# --- 3. Define the Prompt Template ---\ntraining_prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a meticulous math solution verifier. Your task is to carefully analyze the 'Provided Solution' step-by-step to determine if it logically and correctly reaches the 'Expected Answer' for the given 'Question'. Respond ONLY with 'True' if the solution's reasoning AND final result are correct and match the expected answer, otherwise respond ONLY with 'False'.<|eot_id|><|start_header_id|>user<|end_header_id|>\nQuestion:\n{}\n\nExpected Answer:\n{}\n\nProvided Solution:\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{}<|eot_id|>\"\"\"\n\n\n# --- 4. Create a Formatting Function ---\ndef formatting_prompts_func(examples):\n    questions = examples[\"question\"]\n    solutions = examples[\"solution\"]\n    answers = examples[\"answer\"]\n    outputs = [str(x) for x in examples[\"is_correct\"]]\n\n    texts = []\n    for question, solution, answer, output in zip(questions, solutions, answers, outputs):\n        text = training_prompt_template.format(\n            question,\n            str(answer), \n            str(solution),\n            output        \n        )\n        texts.append(text)\n    return { \"text\" : texts }\n\n# -----------------------------------------------------------------\n# --- DEBUG: PRINT THE FINAL FORMATTED PROMPT ---\n# -----------------------------------------------------------------\nprint(\"\\n\" + \"=\"*50)\nprint(\"--- DEBUG: FINAL FORMATTED PROMPT (EXAMPLE 0) ---\")\nprint(\"=\"*50 + \"\\n\")\n# Apply the formatting to just the first example\nformatted_example_text = formatting_prompts_func(train_subset.select(range(1)))['text'][0]\nprint(formatted_example_text)\nprint(\"\\n\" + \"=\"*50)\nprint(\"--- END OF DEBUG ---\")\nprint(\"=\"*50 + \"\\n\")\n\n\n# --- 5. Apply the Formatting (to the whole dataset) ---\nprint(\"Applying formatting to the rest of the dataset...\")\ntry:\n    num_proc = os.cpu_count() // 2 if os.cpu_count() else 4\nexcept:\n    num_proc = 4\nformatted_dataset = train_subset.map(formatting_prompts_func, batched = True, num_proc=num_proc)\n\nprint(\"✅ Dataset loaded and formatted with prompt template.\")\n\n# --- 6. Split into Train and Eval Sets ---\nsplit_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42)\n\nfinal_train_dataset = split_dataset[\"train\"]\nfinal_eval_dataset = split_dataset[\"test\"]\n\nprint(f\"✅ Dataset split into: {len(final_train_dataset)} training examples and {len(final_eval_dataset)} evaluation examples.\")\nprint(\"Cell 8 Complete: Dataset prepared and split.\")\n\n","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3fc0093e28cd4871a855aac834aadcb4","e4784c377c0a4f0f8ff15c35889d5e6b","d99f16a4e908473e897000d33c05433c","698b2a3391254e44b9babbdf0163e440","feca082aa72e47848d7a8cf7063991ce","7ed447e34b4d4a8996c3432dcd2241b3","b44ca299d2ed41c99361178b32d3d2fc","e8238aaa6ebf4db4baa2ac190e01a055","a97f3b7906864868b084ad778c6b5458","86d98b810c8149e6bbabf3ea4487488a","ce864fb137a442dc88c4356247cde238"]},"id":"giRGQD3zE8cC","outputId":"68b5a14d-cc23-405b-cf64-a3e96fc3ff86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using a subset of 2000 examples.\n","\n","==================================================\n","--- DEBUG: RAW DATA (EXAMPLE 0) ---\n","==================================================\n","\n","QUESTION:\n","A line is parameterized by\n","\\[\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} + t \\begin{pmatrix} -1 \\\\ 5 \\end{pmatrix}.\\]A second line is parameterized by\n","\\[\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 7 \\end{pmatrix} + u \\begin{pmatrix} -1 \\\\ 4 \\end{pmatrix}.\\]Find the point where the lines intersect.\n","\n","EXPECTED ANSWER:\n","(2/3,4/3)\n","\n","PROVIDED SOLUTION:\n","First, we need to solve the system of equations\n","\\[\n","\\begin{aligned}\n","2 - t &= s\\\\\n","3 + 5t &= 7 + 4s\n","\\end{aligned}\n","\\]\n","by eliminating s.\n","We'll use sympy.\n","<llm-code>\n","from sympy import symbols, solve\n","\n","# define the variables\n","t, s = symbols('t s')\n","\n","# define the equations\n","equation1 = 2 - t - s\n","equation2 = 3 + 5*t - (7 + 4*s)\n","\n","# solve the equations\n","solutions = solve([equation1, equation2], (t, s))\n","print(solutions)\n","</llm-code>\n","<llm-code-output>\n","{s: 2/3, t: 4/3}\n","</llm-code-output>\n","Therefore the point where the lines intersect is \\boxed{(2/3,4/3)}.\n","\n","IS_CORRECT (Target Label):\n","False\n","\n","\n","==================================================\n","--- DEBUG: FINAL FORMATTED PROMPT (EXAMPLE 0) ---\n","==================================================\n","\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","You are a meticulous math solution verifier. Your task is to carefully analyze the 'Provided Solution' step-by-step to determine if it logically and correctly reaches the 'Expected Answer' for the given 'Question'. Respond ONLY with 'True' if the solution's reasoning AND final result are correct and match the expected answer, otherwise respond ONLY with 'False'.<|eot_id|><|start_header_id|>user<|end_header_id|>\n","Question:\n","A line is parameterized by\n","\\[\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix} + t \\begin{pmatrix} -1 \\\\ 5 \\end{pmatrix}.\\]A second line is parameterized by\n","\\[\\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 7 \\end{pmatrix} + u \\begin{pmatrix} -1 \\\\ 4 \\end{pmatrix}.\\]Find the point where the lines intersect.\n","\n","Expected Answer:\n","(2/3,4/3)\n","\n","Provided Solution:\n","First, we need to solve the system of equations\n","\\[\n","\\begin{aligned}\n","2 - t &= s\\\\\n","3 + 5t &= 7 + 4s\n","\\end{aligned}\n","\\]\n","by eliminating s.\n","We'll use sympy.\n","<llm-code>\n","from sympy import symbols, solve\n","\n","# define the variables\n","t, s = symbols('t s')\n","\n","# define the equations\n","equation1 = 2 - t - s\n","equation2 = 3 + 5*t - (7 + 4*s)\n","\n","# solve the equations\n","solutions = solve([equation1, equation2], (t, s))\n","print(solutions)\n","</llm-code>\n","<llm-code-output>\n","{s: 2/3, t: 4/3}\n","</llm-code-output>\n","Therefore the point where the lines intersect is \\boxed{(2/3,4/3)}.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","False<|eot_id|>\n","\n","==================================================\n","--- END OF DEBUG ---\n","==================================================\n","\n","Applying formatting to the rest of the dataset...\n"]},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=1):   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc0093e28cd4871a855aac834aadcb4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Dataset loaded and formatted with prompt template.\n","✅ Dataset split into: 1800 training examples and 200 evaluation examples.\n","Cell 8 Complete: Dataset prepared and split.\n"]}],"execution_count":12},{"cell_type":"code","source":"%%writefile train.py\n# --- 1. IMPORTS ---\nimport torch\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments, EarlyStoppingCallback, AutoTokenizer\nimport os\nfrom accelerate import Accelerator\nimport pandas as pd\nfrom tqdm import tqdm\n\n\n# --- 2. INITIALIZE ACCELERATOR ---\naccelerator = Accelerator()\nprint(f\"train.py: Process rank: {accelerator.process_index}, Device: {accelerator.device}, Main process: {accelerator.is_main_process}\")\n\n# --- 3. ENVIRONMENT AND SAVE PATH (DYNAMIC) ---\nif 'COLAB_GPU' in os.environ:\n    print(\"train.py: Running in Google Colab.\")\n    print(\"train.py: Saving to local Colab storage.\")\n    SAVE_PATH = \"llama3_math_checkpoint_colab_local\"\nelif os.path.exists('/kaggle/input'):\n    print(\"train.py: Running in Kaggle Notebook.\")\n    SAVE_PATH = \"/kaggle/working/llama3_math_checkpoint\"\nelse:\n    print(\"train.py: Running in a local environment.\")\n    SAVE_PATH = \"llama3_math_checkpoint\"\n\nif accelerator.is_main_process: os.makedirs(SAVE_PATH, exist_ok=True)\naccelerator.wait_for_everyone()\nprint(f\"train.py: Model checkpoint save path: {SAVE_PATH}\")\n\n\n# --- 4. LOAD MODEL AND TOKENIZER ---\nmax_seq_length = 1024\ndtype = None\nload_in_4bit = True\n\nload_model_kwargs = {\n    \"max_seq_length\": max_seq_length,\n    \"dtype\": dtype,\n    \"load_in_4bit\": load_in_4bit,\n}\nif accelerator.num_processes == 1:\n    print(\"Single-GPU setup. Using device_map='auto'.\")\n    load_model_kwargs[\"device_map\"] = \"auto\"\nelse:\n    print(f\"Multi-GPU setup ({accelerator.num_processes} processes). Using accelerate device_map.\")\n    load_model_kwargs[\"device_map\"] = {\"\": accelerator.device}\n\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n    **load_model_kwargs,\n)\nmodel = FastLanguageModel.get_peft_model(\n    model, r=16, lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0, bias=\"none\", use_gradient_checkpointing=\"unsloth\", random_state=42,\n)\n\n# --- 5. PREPARE DATASET ---\ndataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\nshuffled_dataset = dataset.shuffle(seed=42)\n\n# Set to 2000 samples for your test run\ntrain_subset = shuffled_dataset.select(range(2000))\n\ntraining_prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a meticulous math solution verifier. Your task is to carefully analyze the 'Provided Solution' step-by-step to determine if it logically and correctly reaches the 'Expected Answer' for the given 'Question'. Respond ONLY with 'True' if the solution's reasoning AND final result are correct and match the expected answer, otherwise respond ONLY with 'False'.<|eot_id|><|start_header_id|>user<|end_header_id|>\nQuestion:\n{}\n\nExpected Answer:\n{}\n\nProvided Solution:\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{}<|eot_id|>\"\"\"\n\n\ndef formatting_prompts_func(examples):\n    questions = examples[\"question\"]\n    solutions = examples[\"solution\"]\n    answers = examples[\"answer\"]\n    outputs = [str(x) for x in examples[\"is_correct\"]] # Must be strings\n\n    texts = []\n    for question, solution, answer, output in zip(questions, solutions, answers, outputs):\n        text = training_prompt_template.format(\n            question,\n            str(answer),  #\n            str(solution),\n            output        # The target label (\"True\" or \"False\")\n        )\n        texts.append(text)\n    return { \"text\" : texts }\n\ntry: num_proc = os.cpu_count() // 2 if os.cpu_count() else 4\nexcept: num_proc = 4\n\nwith accelerator.main_process_first():\n    formatted_dataset = train_subset.map(formatting_prompts_func, batched=True, num_proc=num_proc)\n    split_dataset = formatted_dataset.train_test_split(test_size=0.1, seed=42) # 10% for eval\n\nfinal_train_dataset = split_dataset[\"train\"]\nfinal_eval_dataset = split_dataset[\"test\"]\nprint(f\"train.py Process {accelerator.process_index}: Dataset prepared: {len(final_train_dataset)} train / {len(final_eval_dataset)} eval examples.\")\n\n\n# --- 6. SET UP TRAINER ---\ntraining_args = TrainingArguments(\n    output_dir=\"training_outputs\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    logging_steps=100,\n    learning_rate=1e-4,\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    seed=42,\n    report_to=\"none\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    save_total_limit=2,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    ddp_find_unused_parameters=False,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=training_args,\n    train_dataset=final_train_dataset,\n    eval_dataset=final_eval_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n    packing=False,\n\n# --- 7. START TRAINING ---\nprint(f\"--- 🚀 Process {accelerator.process_index}: Starting Model Training ---\")\ntrainer.train()\nprint(f\"--- ✅ Process {accelerator.process_index}: Model Training Complete ---\")\n\n# --- 8. SAVE FINAL MODEL (Only on Main Process) ---\naccelerator.wait_for_everyone()\nif accelerator.is_main_process:\n    print(f\"Saving final model checkpoint to {SAVE_PATH}\")\n    trainer.model.save_pretrained(SAVE_PATH)\n    tokenizer.save_pretrained(SAVE_PATH)\n    print(f\"✅ Model adapters and tokenizer saved to: {SAVE_PATH}\")\nelse:\n    print(f\"Process {accelerator.process_index}: Skipping save.\")\naccelerator.wait_for_everyone()\nprint(f\"train.py: Process {accelerator.process_index} finished.\")\n\n","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"BA9aHuumE8cD","outputId":"3bced377-e3da-4dff-b0e2-d085c3fd3767"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting train.py\n"]}],"execution_count":10},{"cell_type":"code","source":"print(\"Cell 10: Launching training script...\")\nimport torch\nimport os\nif torch.cuda.is_available():\n    num_processes = torch.cuda.device_count()\n    print(f\"Found {num_processes} GPU(s). Launching with {num_processes} processes.\")\n    command = f\"accelerate launch --num_processes={num_processes} train.py\"\nelse:\n    num_processes = 1\n    print(\"No GPU found. Launching with 1 CPU process.\")\n    command = f\"accelerate launch --num_processes={num_processes} --cpu train.py\"\n\nprint(f\"Running command: {command}\")\n\n!{command}\n\nprint(\"Cell 10 Complete: Training script finished.\")","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Rpy1Gv6UE8cD","outputId":"3862dfe3-1b08-4a63-c4d3-dcaea14427c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cell 10: Launching training script...\n","Found 1 GPU(s). Launching with 1 processes.\n","Running command: accelerate launch --num_processes=1 train.py\n","The following values were not passed to `accelerate launch` and had defaults used instead:\n","\t`--num_machines` was set to a value of `1`\n","\t`--mixed_precision` was set to a value of `'no'`\n","\t`--dynamo_backend` was set to a value of `'no'`\n","To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n","🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","2025-11-01 16:32:50.349522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1762014770.421465    4631 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1762014770.433129    4631 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1762014770.457606    4631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1762014770.457645    4631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1762014770.457655    4631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1762014770.457662    4631 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","[torchao|WARNING]Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu126 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n","🦥 Unsloth Zoo will now patch everything to make training faster!\n","train.py: Process rank: 0, Device: cuda, Main process: True\n","train.py: Running in Google Colab.\n","train.py: Saving to local Colab storage.\n","train.py: Model checkpoint save path: llama3_math_checkpoint_colab_local\n","Single-GPU setup. Using device_map='auto'.\n","==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","Unsloth 2025.10.12 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","Map (num_proc=1): 100% 2000/2000 [00:00<00:00, 2569.24 examples/s]\n","train.py Process 0: Dataset prepared: 1800 train / 200 eval examples.\n","Unsloth: Tokenizing [\"text\"] (num_proc=6): 100% 1800/1800 [00:07<00:00, 232.96 examples/s]\n","Unsloth: Tokenizing [\"text\"] (num_proc=6): 100% 200/200 [00:05<00:00, 38.52 examples/s]\n","--- 🚀 Process 0: Starting Model Training ---\n","The model is already on multiple devices. Skipping the move to device specified in `args`.\n","==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 1,800 | Num Epochs = 1 | Total steps = 225\n","O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n","\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n"," \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n","  0% 0/225 [00:00<?, ?it/s]Unsloth: Will smartly offload gradients to save VRAM!\n"," 32% 73/225 [12:32<24:18,  9.59s/it]Traceback (most recent call last):\n","  File \"/usr/lib/python3.12/subprocess.py\", line 1264, in wait\n","    return self._wait(timeout=timeout)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/subprocess.py\", line 2053, in _wait\n","    (pid, sts) = self._try_wait(0)\n","                 ^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.12/subprocess.py\", line 2011, in _try_wait\n","    (pid, sts) = os.waitpid(self.pid, wait_flags)\n","                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/accelerate\", line 10, in <module>\n","    sys.exit(main())\n","             ^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n","    args.func(args)\n","  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1235, in launch_command\n","    simple_launcher(args)\n","  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 820, in simple_launcher\n","    process.wait()\n","  File \"/usr/lib/python3.12/subprocess.py\", line 1277, in wait\n","    self._wait(timeout=sigint_timeout)\n","  File \"/usr/lib/python3.12/subprocess.py\", line 2047, in _wait\n","    time.sleep(delay)\n","KeyboardInterrupt\n","Traceback (most recent call last):\n","  File \"/content/train.py\", line 164, in <module>\n","    trainer.train()\n","  File \"/content/unsloth_compiled_cache/UnslothSFTTrainer.py\", line 53, in wrapper\n","    output = f(self, *args, **kwargs)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 2325, in train\n","    return inner_training_loop(\n","           ^^^^^^^^^^^^^^^^^^^^\n","  File \"<string>\", line 335, in _fast_inner_training_loop\n","KeyboardInterrupt\n","Cell 10 Complete: Training script finished.\n"]}],"execution_count":11},{"cell_type":"code","source":"# Import garbage collector and clear PyTorch's cache\nimport gc\nimport torch\nimport os\nimport glob \ntry:\n    del model\n    del tokenizer\n    print(\"Deleted notebook's model and tokenizer objects.\")\nexcept NameError:\n    print(\"Model/Tokenizer already deleted or not defined in this notebook scope.\")\n\ngc.collect() \nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"Cleared notebook memory. Loading trained model from checkpoint for inference...\")\n\n# --- Load the Final Saved Model ---\nCHECKPOINT_PATH = \"llama3_math_checkpoint_colab_local\"\n\nif not os.path.isdir(CHECKPOINT_PATH):\n    raise RuntimeError(\n        f\"ERROR: The checkpoint path '{CHECKPOINT_PATH}' was not found. \"\n        \"Did Cell 10 (training) run successfully and save the model?\"\n    )\n\nprint(f\"Loading final trained model from: {CHECKPOINT_PATH}\")\n\n# --- Load the saved model --\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = CHECKPOINT_PATH,\n    max_seq_length = 1024,\n    dtype = None,\n    load_in_4bit = True,\n)\n\n# --- Prepare for Inference ---\nFastLanguageModel.for_inference(model)\n\nprint(\"✅ Model loaded from checkpoint and prepared for inference.\")\nprint(\"Cell 11 Complete: Memory cleared and trained model reloaded.\")\n\n","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"bwkeMhmJE8cD","outputId":"85591c91-5c5a-4eb2-a6e7-cae0424fa325"},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted notebook's model and tokenizer objects.\n","Cleared notebook memory. Loading trained model from checkpoint for inference...\n","Loading model from: training_outputs/checkpoint-1350\n","==((====))==  Unsloth 2025.10.11: Fast Llama patching. Transformers: 4.57.1.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","✅ Model loaded from checkpoint and prepared for inference.\n","Cell 11 Complete: Memory cleared and trained model reloaded.\n"]}],"execution_count":null},{"cell_type":"code","source":"# --- 1. Load the Test Dataset ---\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ntest_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n\n# --- 2. Define the Llama 3 Inference Prompt ---\ninference_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a meticulous math solution verifier. Your task is to carefully analyze the 'Provided Solution' step-by-step to determine if it logically and correctly reaches the 'Expected Answer' for the given 'Question'. Respond ONLY with 'True' if the solution's reasoning AND final result are correct and match the expected answer, otherwise respond ONLY with 'False'.<|eot_id|><|start_header_id|>user<|end_header_id|>\nQuestion:\n{}\nSolution:\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\n# --- 3. Define an Output Parser (Updated) ---\ndef parse_output(response_text):\n    cleaned_output = response_text.strip()\n\n    if cleaned_output.lower().startswith('true'):\n        return True\n    elif cleaned_output.lower().startswith('false'):\n        return False\n    else:\n        print(f\"Warning: Unexpected output format: '{cleaned_output[:50]}...' - Defaulting to False\")\n        return False\n\n# --- 4. Prediction Loop ---\npredictions = []\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Generating predictions on the test set using device: {device}...\")\nfor example in tqdm(test_dataset):\n    question = example[\"question\"]\n    answer = str(example[\"answer\"])\n    solution = str(example[\"solution\"])\n    prompt = inference_prompt.format(question, answer, solution)\n\n    inputs = tokenizer([prompt], return_tensors = \"pt\", truncation=True, max_length=max_seq_length).to(device)\n\n    # Generate the model's response\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens = 8,\n            use_cache = True,\n            pad_token_id = tokenizer.eos_token_id\n        )\n    input_length = inputs.input_ids.shape[1]\n    generated_token_ids = outputs[0][input_length:]\n    response_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n\n    prediction = parse_output(response_text)\n    predictions.append(prediction)\n\nprint(\"✅ All predictions generated.\")\nprint(\"Cell 12 Complete: Predictions generated.\")\n","metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-MICKYkVE8cE","outputId":"8bd95284-498b-431c-b482-a865121c8e84"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generating predictions on the test set using device: cuda...\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/10000 [00:00<2:37:49,  1.06it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's solve this problem using...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 2/10000 [00:01<2:28:04,  1.13it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's use sympy to...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 3/10000 [00:02<2:32:49,  1.09it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's solve this problem using...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 4/10000 [00:03<2:38:28,  1.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's solve this problem using...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 5/10000 [00:04<2:28:32,  1.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's write down the equation...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 6/10000 [00:05<2:23:35,  1.16it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","We need to find the values...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 7/10000 [00:06<2:16:45,  1.22it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's use sympy to...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 8/10000 [00:06<2:15:16,  1.23it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's use sympy to...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 9/10000 [00:07<2:13:23,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","We will use Python's symp...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 10/10000 [00:08<2:11:42,  1.26it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's first compute the probability...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 11/10000 [00:09<2:13:57,  1.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's use sympy to...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 12/10000 [00:10<2:13:54,  1.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","The intersection of a plane with...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 13/10000 [00:10<2:13:06,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","We can use a recursive function...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 14/10000 [00:11<2:09:39,  1.28it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","We can use the Python's...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 15/10000 [00:12<2:09:06,  1.29it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's solve this problem using...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 16/10000 [00:13<2:06:36,  1.31it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","<llm-code>\n","# We...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 17/10000 [00:13<2:14:31,  1.24it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","There are 26 letters and...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 18/10000 [00:14<2:19:36,  1.19it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","We can use the following formula...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 19/10000 [00:15<2:26:13,  1.14it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","We will use Python's symp...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 20/10000 [00:16<2:28:44,  1.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Let's first find the angle that All...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 21/10000 [00:17<2:23:47,  1.16it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's use the law of...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 22/10000 [00:18<2:19:27,  1.19it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's solve this problem using...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 23/10000 [00:19<2:23:24,  1.16it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let $r$ and $...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 24/10000 [00:20<2:15:48,  1.22it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","To find the number of multiples...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 25/10000 [00:20<2:13:02,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's solve this problem using...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 26/10000 [00:21<2:13:08,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","We can use sympy to...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 27/10000 [00:22<2:23:48,  1.16it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","The distance between $P$...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 28/10000 [00:23<2:25:39,  1.14it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","In the given diagram, we...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 29/10000 [00:24<2:17:53,  1.21it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let the length of the rectangle...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 30/10000 [00:24<2:12:57,  1.25it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's solve this problem using...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 31/10000 [00:25<2:12:11,  1.26it/s]"]},{"output_type":"stream","name":"stdout","text":["Warning: Unexpected output format: 'Output:\n","Let's solve this problem using...' - Defaulting to False\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 31/10000 [00:25<2:19:10,  1.19it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2091281315.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# We set pad_token_id to eos_token_id to prevent warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 8 tokens is enough for \"True\" or \"False\" + EOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36munsloth_fast_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1772\u001b[0m     \u001b[0;31m# Mixed precision autocast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEVICE_TYPE_TORCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1774\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1775\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36m_CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_no_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             outputs = self.model(\n\u001b[0m\u001b[1;32m   1158\u001b[0m                 \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mcausal_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    970\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0mcausal_mask\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfast_rms_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    642\u001b[0m             \u001b[0mhidden_states\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mcausal_mask\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mLlamaAttention_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;31m# Clear inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"paged_attention\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaged_attention_K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaged_attention_V\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaged_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__delattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2075\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2076\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_register_state_dict_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":null},{"cell_type":"code","source":"# --- 1. Create a Pandas DataFrame ---\nsubmission = pd.DataFrame({\n    'ID': range(len(predictions)),\n    'is_correct': predictions\n})\n\n# --- 2. Save to CSV ---\nsubmission_path = 'submission.csv'\nsubmission.to_csv(submission_path, index=False)\n\nprint(f\"\\n🎉 Submission file '{submission_path}' created successfully!\")\n\n# --- 3. Display the first 5 rows ---\nprint(\"\\nFirst 5 predictions:\")\nprint(submission.head())\nprint(f\"\\nTotal predictions: {len(submission)}\")\nprint(f\"Count of True predictions: {submission['is_correct'].sum()}\")\nprint(f\"Count of False predictions: {len(submission) - submission['is_correct'].sum()}\")\nprint(\"Cell 13 Complete: Submission file created.\")","metadata":{"trusted":true,"id":"s1D7Mgp_E8cE"},"outputs":[],"execution_count":null}]}